{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMHbj34G2rlSiAeHinEnXK1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ankitBawanthade123/VisualQuestionAnsweringProject/blob/main/RunOperations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utQBWPNloCNX",
        "outputId": "cc7ed678-6853-4a82-95ce-dd4023bb2f1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CFR_VQA'...\n",
            "remote: Enumerating objects: 69, done.\u001b[K\n",
            "remote: Counting objects: 100% (69/69), done.\u001b[K\n",
            "remote: Compressing objects: 100% (62/62), done.\u001b[K\n",
            "remote: Total 69 (delta 17), reused 45 (delta 6), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (69/69), 271.03 KiB | 9.35 MiB/s, done.\n",
            "Resolving deltas: 100% (17/17), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/aioz-ai/CFR_VQA.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd CFR_VQA/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zp7C3rQvoTMw",
        "outputId": "ce4d9782-5c47-4520-a7e9-25d49eb0a7e2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/CFR_VQA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NbfPF1_QoVkZ",
        "outputId": "5cc2ded1-30ca-45ef-d98d-688609102cb1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting h5py==2.9.0 (from -r requirements.txt (line 1))\n",
            "  Downloading h5py-2.9.0.tar.gz (287 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/287.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.9/287.9 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting matplotlib==3.1.0 (from -r requirements.txt (line 2))\n",
            "  Downloading matplotlib-3.1.0.tar.gz (37.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.2/37.2 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.1.0 (from versions: 1.11.0, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 2.0.0, 2.0.1, 2.1.0, 2.1.1, 2.1.2, 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0, 2.4.1, 2.5.0, 2.5.1)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==1.1.0\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://vision.aioz.io/f/c11580c0318846d1939c/?dl=1 -O gqa_dataset.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAmOi492oXpG",
        "outputId": "5b41fd7d-cae7-4d95-db65-e0f8aa0dadbd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-01 18:12:13--  https://vision.aioz.io/f/c11580c0318846d1939c/?dl=1\n",
            "Resolving vision.aioz.io (vision.aioz.io)... 115.79.141.245\n",
            "Connecting to vision.aioz.io (vision.aioz.io)|115.79.141.245|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://vision.aioz.io/seafhttp/files/ae346230-73f2-45fc-aa19-079222a464cf/gqa.zip [following]\n",
            "--2024-12-01 18:12:14--  https://vision.aioz.io/seafhttp/files/ae346230-73f2-45fc-aa19-079222a464cf/gqa.zip\n",
            "Reusing existing connection to vision.aioz.io:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 22900522729 (21G) [application/zip]\n",
            "Saving to: ‘gqa_dataset.zip’\n",
            "\n",
            "gqa_dataset.zip     100%[===================>]  21.33G  7.86MB/s    in 36m 27s \n",
            "\n",
            "2024-12-01 18:48:42 (9.98 MB/s) - ‘gqa_dataset.zip’ saved [22900522729/22900522729]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ..\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npMNoo80slAe",
        "outputId": "a38fe5bb-b4ee-4ba8-c2c0-69bdd8f43ba4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/CFR_VQA/gqa_dataset.zip -d /content/CFR_VQA/data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4wo90RSsocU",
        "outputId": "f8f54426-b4c1-4408-bf73-6e7ad1700b46"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/CFR_VQA/gqa_dataset.zip\n",
            "   creating: /content/CFR_VQA/data/gqa/\n",
            "  inflating: /content/CFR_VQA/data/gqa/v2_mscoco_train2014_annotations.json  \n",
            "  inflating: /content/CFR_VQA/data/gqa/train_5_stats_words.json  \n",
            "  inflating: /content/CFR_VQA/data/gqa/train_predicates.json  \n",
            "  inflating: /content/CFR_VQA/data/gqa/val.hdf5  \n",
            "  inflating: /content/CFR_VQA/data/gqa/train_6_stats_skip_imgid.json  \n",
            "  inflating: /content/CFR_VQA/data/gqa/image_data.json  \n",
            "  inflating: /content/CFR_VQA/data/gqa/val_5_stats_words.json  \n",
            "  inflating: /content/CFR_VQA/data/gqa/test_balanced_questions.json  \n",
            "  inflating: /content/CFR_VQA/data/gqa/gqa_val_questions_entities.json  \n",
            "  inflating: /content/CFR_VQA/data/gqa/val_ids.pkl  \n",
            "  inflating: /content/CFR_VQA/data/gqa/test_imgid2idx.pkl  \n",
            "  inflating: /content/CFR_VQA/data/gqa/val_6_stats_words.json  \n",
            "  inflating: /content/CFR_VQA/data/gqa/train_ids.pkl  \n",
            "  inflating: /content/CFR_VQA/data/gqa/test_stat_non_plural_skip_imgid.json  \n",
            "  inflating: /content/CFR_VQA/data/gqa/test_5_stats_words.json  \n",
            "  inflating: /content/CFR_VQA/data/gqa/challenge.hdf5  \n",
            "  inflating: /content/CFR_VQA/data/gqa/v2_OpenEnded_mscoco_train2014_questions_entities_reverse.json  \n",
            "  inflating: /content/CFR_VQA/data/gqa/train_fpn.hdf5  \n",
            "  inflating: /content/CFR_VQA/data/gqa/train_attr_words_non_plural_words.json  \n",
            "  inflating: /content/CFR_VQA/data/gqa/val_6_stats_skip_imgid.json  \n",
            "  inflating: /content/CFR_VQA/data/gqa/test_attr_words_non_plural_words.json  \n",
            "  inflating: /content/CFR_VQA/data/gqa/train_6_stats_words.json  \n",
            "  inflating: /content/CFR_VQA/data/gqa/glove6b_init_300d.npy  \n",
            "  inflating: /content/CFR_VQA/data/gqa/val_balanced_questions.json  \n",
            "  inflating: /content/CFR_VQA/data/gqa/train_balanced_questions.json  \n",
            "  inflating: /content/CFR_VQA/data/gqa/v2_OpenEnded_mscoco_train2014_questions_10.json  \n",
            "  inflating: /content/CFR_VQA/data/gqa/v2_mscoco_val2014_annotations.json  \n",
            "  inflating: /content/CFR_VQA/data/gqa/test_6_stats_skip_imgid.json  \n",
            "  inflating: /content/CFR_VQA/data/gqa/v2_OpenEnded_mscoco_val2014_questions_entities_reverse.json  \n",
            "  inflating: /content/CFR_VQA/data/gqa/val_attr_skip_imgid.json  \n",
            "  inflating: /content/CFR_VQA/data/gqa/val_predicates_old.json  \n",
            "  inflating: /content/CFR_VQA/data/gqa/challenge_ids.pkl  \n",
            "  inflating: /content/CFR_VQA/data/gqa/val_stat_non_plural_words.json  \n",
            "  inflating: /content/CFR_VQA/data/gqa/v2_OpenEnded_mscoco_test_questions.json  \n",
            "  inflating: /content/CFR_VQA/data/gqa/v2_OpenEnded_mscoco_val2014_questions.json  \n",
            "  inflating: /content/CFR_VQA/data/gqa/val_teacher_logits.pkl  \n",
            "  inflating: /content/CFR_VQA/data/gqa/val_predicates.json  \n",
            "  inflating: /content/CFR_VQA/data/gqa/train_predicates_old.json  \n",
            "  inflating: /content/CFR_VQA/data/gqa/train_imgid2idx.pkl  \n",
            "  inflating: /content/CFR_VQA/data/gqa/v2_OpenEnded_mscoco_challenge2014_questions.json  \n",
            "  inflating: /content/CFR_VQA/data/gqa/val_fpn.hdf5  \n",
            "  inflating: /content/CFR_VQA/data/gqa/v2_OpenEnded_mscoco_val2014_questions_10.json  \n",
            "  inflating: /content/CFR_VQA/data/gqa/test_predicates.json  \n",
            "   creating: /content/CFR_VQA/data/gqa/glove/\n",
            "  inflating: /content/CFR_VQA/data/gqa/glove/glove.6B.300d.txt  \n",
            "  inflating: /content/CFR_VQA/data/gqa/challenge_imgid2idx.pkl  \n",
            "  inflating: /content/CFR_VQA/data/gqa/v2_OpenEnded_mscoco_train2014_questions.json  \n",
            "  inflating: /content/CFR_VQA/data/gqa/val_stat_non_plural_skip_imgid.json  \n",
            "  inflating: /content/CFR_VQA/data/gqa/train_add_word_obj.json  \n",
            "  inflating: /content/CFR_VQA/data/gqa/dictionary.pkl  \n",
            "  inflating: /content/CFR_VQA/data/gqa/test_ids.pkl  \n",
            "  inflating: /content/CFR_VQA/data/gqa/train_attr_skip_imgid.json  \n",
            "  inflating: /content/CFR_VQA/data/gqa/test_add_word_obj.json  \n",
            "  inflating: /content/CFR_VQA/data/gqa/challenge_balanced_questions.json  \n",
            "  inflating: /content/CFR_VQA/data/gqa/gqa_train_questions_entities.json  \n",
            "  inflating: /content/CFR_VQA/data/gqa/test.hdf5  \n",
            "  inflating: /content/CFR_VQA/data/gqa/train_teacher_logits.pkl  \n",
            "  inflating: /content/CFR_VQA/data/gqa/val_add_word_obj.json  \n",
            "   creating: /content/CFR_VQA/data/gqa/cache/\n",
            "  inflating: /content/CFR_VQA/data/gqa/cache/trainval_label2ans.pkl  \n",
            "  inflating: /content/CFR_VQA/data/gqa/cache/val_target.pkl  \n",
            "  inflating: /content/CFR_VQA/data/gqa/cache/trainval_ans2label.pkl  \n",
            "  inflating: /content/CFR_VQA/data/gqa/cache/train_target.pkl  \n",
            "  inflating: /content/CFR_VQA/data/gqa/v2_OpenEnded_mscoco_test_questions_entities_reverse.json  \n",
            "  inflating: /content/CFR_VQA/data/gqa/gqa_test_questions_entities.json  \n",
            "  inflating: /content/CFR_VQA/data/gqa/val_imgid2idx.pkl  \n",
            "  inflating: /content/CFR_VQA/data/gqa/embed_tfidf_weights.pkl  \n",
            "  inflating: /content/CFR_VQA/data/gqa/test_stat_non_plural_words.json  \n",
            "  inflating: /content/CFR_VQA/data/gqa/train_stat_non_plural_words.json  \n",
            "  inflating: /content/CFR_VQA/data/gqa/question_answers.json  \n",
            "  inflating: /content/CFR_VQA/data/gqa/train_stat_non_plural_skip_imgid.json  \n",
            "  inflating: /content/CFR_VQA/data/gqa/val_attr_words_non_plural_words.json  \n",
            "  inflating: /content/CFR_VQA/data/gqa/test_attr_skip_imgid.json  \n",
            "  inflating: /content/CFR_VQA/data/gqa/test_6_stats_words.json  \n",
            "  inflating: /content/CFR_VQA/data/gqa/train.hdf5  \n",
            "   creating: /content/CFR_VQA/data/gqa/lxmert/\n",
            "  inflating: /content/CFR_VQA/data/gqa/lxmert/all_ans.json  \n",
            "  inflating: /content/CFR_VQA/data/gqa/lxmert/vgnococo.json  \n",
            "  inflating: /content/CFR_VQA/data/gqa/lxmert/vg_nococo_imgid.json  \n",
            "  inflating: /content/CFR_VQA/data/gqa/lxmert/mscoco_train.json  \n",
            "  inflating: /content/CFR_VQA/data/gqa/lxmert/ans2lbl.pkl  \n",
            "  inflating: /content/CFR_VQA/data/gqa/lxmert/mscoco_minival.json  \n",
            "  inflating: /content/CFR_VQA/data/gqa/lxmert/mscoco_nominival.json  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install boto3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fabnvV2aymR4",
        "outputId": "0bd1ab43-e533-49dc-92e0-905ad8a6952a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting boto3\n",
            "  Downloading boto3-1.35.71-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting botocore<1.36.0,>=1.35.71 (from boto3)\n",
            "  Downloading botocore-1.35.71-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3)\n",
            "  Downloading s3transfer-0.10.4-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.36.0,>=1.35.71->boto3) (2.8.2)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.36.0,>=1.35.71->boto3) (2.2.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.36.0,>=1.35.71->boto3) (1.16.0)\n",
            "Downloading boto3-1.35.71-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.35.71-py3-none-any.whl (13.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m79.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading s3transfer-0.10.4-py3-none-any.whl (83 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.2/83.2 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jmespath, botocore, s3transfer, boto3\n",
            "Successfully installed boto3-1.35.71 botocore-1.35.71 jmespath-1.0.1 s3transfer-0.10.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yoLVCOBy3xp",
        "outputId": "8f58b486-8fd6-49cd-a7ef-03616679a0ee"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd CFR_VQA/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rrg0sxypy9NC",
        "outputId": "8d1df251-7c5c-4ac7-ba7d-b875d42d58c3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/CFR_VQA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir pretrained"
      ],
      "metadata": {
        "id": "BPjj5PbLvuhi"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://vision.aioz.io/f/2f6316d1b8794079b913/?dl=1 -O pretrained/model_LXRT.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WMYjQlHv2GP",
        "outputId": "fa49662a-e144-474e-c992-ffe653a6fc15"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-01 19:19:55--  https://vision.aioz.io/f/2f6316d1b8794079b913/?dl=1\n",
            "Resolving vision.aioz.io (vision.aioz.io)... 115.79.141.245\n",
            "Connecting to vision.aioz.io (vision.aioz.io)|115.79.141.245|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://vision.aioz.io/seafhttp/files/a9f9da87-cc3a-427a-b92f-516a8f110728/model_LXRT.pth [following]\n",
            "--2024-12-01 19:19:56--  https://vision.aioz.io/seafhttp/files/a9f9da87-cc3a-427a-b92f-516a8f110728/model_LXRT.pth\n",
            "Reusing existing connection to vision.aioz.io:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 912336661 (870M) [application/octet-stream]\n",
            "Saving to: ‘pretrained/model_LXRT.pth’\n",
            "\n",
            "pretrained/model_LX 100%[===================>] 870.07M  10.2MB/s    in 85s     \n",
            "\n",
            "2024-12-01 19:21:21 (10.3 MB/s) - ‘pretrained/model_LXRT.pth’ saved [912336661/912336661]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash train.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkNUQbhVzBUk",
        "outputId": "38c44565-637f-4769-9604-9ab39c97359c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(input=None, output='saved_models/GQA/replicate_result_default', seed=1204, epochs=13, lr=0.0007, batch_size=64, update_freq='4', use_both=False, model='CFRF_Model', dataset='GQA', gamma=2, use_counter=False, counter_act='zhang', testing=False, print_interval=200, gpu=0, clip_norm=0.25, weight_init='none', max_boxes=50, num_stat_word=30, question_len=12, tfidf=True, op='c', num_hid=1024, activation='swish', dropout=0.45, fast=False, tiny=False, tqdm=False, load=None, load_lxmert=None, load_lxmert_qa='pretrained/model', mce_loss=False, lxmert_lr=0.0001, llayers=7, xlayers=2, rlayers=2, task_matched=False, task_mask_lm=False, task_obj_predict=False, task_qa=False, visual_losses='obj,attr,feat', qa_sets=None, word_mask_rate=0.15, obj_mask_rate=0.15, multiGPU=False, num_workers=0, omega_q=0.1, omega_v=0.01, fusion_ratio=0.1, topk=6)\n",
            "loading dictionary from data/gqa/dictionary.pkl\n",
            "Create train entries\n",
            "loading features from h5 file data/gqa/train.hdf5 \n",
            "Create val entries\n",
            "loading features from h5 file data/gqa/val.hdf5 \n",
            "LXRT encoder with 7 l_layers, 2 x_layers, and 2 r_layers.\n",
            "/content/CFR_VQA/lxrt/modeling.py:843: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(weights_path, map_location='cpu' if not torch.cuda.is_available() else None)\n",
            "Load QA pre-trained LXMERT from pretrained/model \n",
            "/content/CFR_VQA/pretrain/qa_answer_table.py:172: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  loaded_state_dict = torch.load(\"%s_LXRT.pth\" % path, map_location=device)\n",
            "Loaded 1528 answers from LXRTQA pre-training and 5 not\n",
            "\n",
            "loading dictionary from data/gqa/dictionary.pkl\n",
            "Loading embedding tfidf and weights from file\n",
            "/content/CFR_VQA/src/utils.py:400: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  w_emb = torch.load(f)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/serialization.py:1469: SourceChangeWarning: source code of class 'language_model.WordEmbedding' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/serialization.py:1469: SourceChangeWarning: source code of class 'torch.nn.modules.sparse.Embedding' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/serialization.py:1469: SourceChangeWarning: source code of class 'torch.nn.modules.dropout.Dropout' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "Load embedding tfidf and weights from file successfully\n",
            "loading dictionary from data/gqa/dictionary.pkl\n",
            "Loading embedding tfidf and weights from file\n",
            "Load embedding tfidf and weights from file successfully\n",
            "loading dictionary from data/gqa/dictionary.pkl\n",
            "Loading embedding tfidf and weights from file\n",
            "Load embedding tfidf and weights from file successfully\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
            "  WeightNorm.apply(module, name, dim)\n",
            "Namespace(input=None, output='saved_models/GQA/replicate_result_default', seed=1204, epochs=13, lr=0.0007, batch_size=64, update_freq='4', use_both=False, model='CFRF_Model', dataset='GQA', gamma=2, use_counter=False, counter_act='zhang', testing=False, print_interval=200, gpu=0, clip_norm=0.25, weight_init='none', max_boxes=50, num_stat_word=30, question_len=12, tfidf=True, op='c', num_hid=1024, activation='swish', dropout=0.45, fast=False, tiny=False, tqdm=False, load=None, load_lxmert=None, load_lxmert_qa='pretrained/model', mce_loss=False, lxmert_lr=0.0001, llayers=7, xlayers=2, rlayers=2, task_matched=False, task_mask_lm=False, task_obj_predict=False, task_qa=False, visual_losses='obj,attr,feat', qa_sets=None, word_mask_rate=0.15, obj_mask_rate=0.15, multiGPU=False, num_workers=0, omega_q=0.1, omega_v=0.01, fusion_ratio=0.1, topk=6, device=device(type='cuda', index=0))\n",
            "CFRF_Model(\n",
            "  (w_emb): WordEmbedding(\n",
            "    (emb): Embedding(2932, 300, padding_idx=2931)\n",
            "    (emb_): Embedding(2932, 300, padding_idx=2931)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (q_emb): QuestionEmbedding(\n",
            "    (rnn): GRU(600, 1024, batch_first=True)\n",
            "  )\n",
            "  (sw_emb): WordEmbedding(\n",
            "    (emb): Embedding(2932, 300, padding_idx=2931)\n",
            "    (emb_): Embedding(2932, 300, padding_idx=2931)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (s_emb): QuestionEmbedding(\n",
            "    (rnn): GRU(600, 1024, batch_first=True)\n",
            "  )\n",
            "  (ew_emb): WordEmbedding(\n",
            "    (emb): Embedding(2932, 300, padding_idx=2931)\n",
            "    (emb_): Embedding(2932, 300, padding_idx=2931)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (qe_joint): BanFusion(\n",
            "    (v_att): BiAttention(\n",
            "      (logits): BCNet(\n",
            "        (v_net): FCNet(\n",
            "          (main): Sequential(\n",
            "            (0): Dropout(p=0.2, inplace=False)\n",
            "            (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (2): ReLU()\n",
            "          )\n",
            "        )\n",
            "        (q_net): FCNet(\n",
            "          (main): Sequential(\n",
            "            (0): Dropout(p=0.2, inplace=False)\n",
            "            (1): Linear(in_features=600, out_features=1024, bias=True)\n",
            "            (2): ReLU()\n",
            "          )\n",
            "        )\n",
            "        (dropout): Dropout(p=0.5, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (b_net): ModuleList(\n",
            "      (0-1): 2 x BCNet(\n",
            "        (v_net): FCNet(\n",
            "          (main): Sequential(\n",
            "            (0): Dropout(p=0.2, inplace=False)\n",
            "            (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (2): ReLU()\n",
            "          )\n",
            "        )\n",
            "        (q_net): FCNet(\n",
            "          (main): Sequential(\n",
            "            (0): Dropout(p=0.2, inplace=False)\n",
            "            (1): Linear(in_features=600, out_features=1024, bias=True)\n",
            "            (2): ReLU()\n",
            "          )\n",
            "        )\n",
            "        (dropout): Dropout(p=0.5, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (q_prj): ModuleList(\n",
            "      (0-1): 2 x FCNet(\n",
            "        (main): Sequential(\n",
            "          (0): Dropout(p=0.2, inplace=False)\n",
            "          (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (vs_joint): BanFusion(\n",
            "    (v_att): BiAttention(\n",
            "      (logits): BCNet(\n",
            "        (v_net): FCNet(\n",
            "          (main): Sequential(\n",
            "            (0): Dropout(p=0.2, inplace=False)\n",
            "            (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (2): ReLU()\n",
            "          )\n",
            "        )\n",
            "        (q_net): FCNet(\n",
            "          (main): Sequential(\n",
            "            (0): Dropout(p=0.2, inplace=False)\n",
            "            (1): Linear(in_features=2048, out_features=1024, bias=True)\n",
            "            (2): ReLU()\n",
            "          )\n",
            "        )\n",
            "        (dropout): Dropout(p=0.5, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (b_net): ModuleList(\n",
            "      (0-1): 2 x BCNet(\n",
            "        (v_net): FCNet(\n",
            "          (main): Sequential(\n",
            "            (0): Dropout(p=0.2, inplace=False)\n",
            "            (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (2): ReLU()\n",
            "          )\n",
            "        )\n",
            "        (q_net): FCNet(\n",
            "          (main): Sequential(\n",
            "            (0): Dropout(p=0.2, inplace=False)\n",
            "            (1): Linear(in_features=2048, out_features=1024, bias=True)\n",
            "            (2): ReLU()\n",
            "          )\n",
            "        )\n",
            "        (dropout): Dropout(p=0.5, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (q_prj): ModuleList(\n",
            "      (0-1): 2 x FCNet(\n",
            "        (main): Sequential(\n",
            "          (0): Dropout(p=0.2, inplace=False)\n",
            "          (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (vq_joint): BanFusion(\n",
            "    (v_att): BiAttention(\n",
            "      (logits): BCNet(\n",
            "        (v_net): FCNet(\n",
            "          (main): Sequential(\n",
            "            (0): Dropout(p=0.2, inplace=False)\n",
            "            (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (2): ReLU()\n",
            "          )\n",
            "        )\n",
            "        (q_net): FCNet(\n",
            "          (main): Sequential(\n",
            "            (0): Dropout(p=0.2, inplace=False)\n",
            "            (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (2): ReLU()\n",
            "          )\n",
            "        )\n",
            "        (dropout): Dropout(p=0.5, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (b_net): ModuleList(\n",
            "      (0-1): 2 x BCNet(\n",
            "        (v_net): FCNet(\n",
            "          (main): Sequential(\n",
            "            (0): Dropout(p=0.2, inplace=False)\n",
            "            (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (2): ReLU()\n",
            "          )\n",
            "        )\n",
            "        (q_net): FCNet(\n",
            "          (main): Sequential(\n",
            "            (0): Dropout(p=0.2, inplace=False)\n",
            "            (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (2): ReLU()\n",
            "          )\n",
            "        )\n",
            "        (dropout): Dropout(p=0.5, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (q_prj): ModuleList(\n",
            "      (0-1): 2 x FCNet(\n",
            "        (main): Sequential(\n",
            "          (0): Dropout(p=0.2, inplace=False)\n",
            "          (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (lxmert_encoder): Lxmert_Model(\n",
            "    (lxrt_encoder): LXRTEncoder(\n",
            "      (model): LXRTFeatureExtraction(\n",
            "        (bert): LXRTModel(\n",
            "          (embeddings): BertEmbeddings(\n",
            "            (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "            (position_embeddings): Embedding(512, 768, padding_idx=0)\n",
            "            (token_type_embeddings): Embedding(2, 768, padding_idx=0)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (encoder): LXRTEncoder(\n",
            "            (visn_fc): VisualFeatEncoder(\n",
            "              (visn_fc): Linear(in_features=2048, out_features=768, bias=True)\n",
            "              (visn_layer_norm): BertLayerNorm()\n",
            "              (box_fc): Linear(in_features=4, out_features=768, bias=True)\n",
            "              (box_layer_norm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (layer): ModuleList(\n",
            "              (0-6): 7 x BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): BertLayerNorm()\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): BertLayerNorm()\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (x_layers): ModuleList(\n",
            "              (0-1): 2 x LXRTXLayer(\n",
            "                (lang_self_att): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): BertLayerNorm()\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (lang_inter): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (lang_output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): BertLayerNorm()\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (visn_self_att): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): BertLayerNorm()\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (visn_inter): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (visn_output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): BertLayerNorm()\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (visual_attention): BertXAttention(\n",
            "                  (att): BertOutAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): BertLayerNorm()\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (r_layers): ModuleList(\n",
            "              (0-1): 2 x BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): BertLayerNorm()\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): BertLayerNorm()\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (pooler): BertPooler(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (activation): Tanh()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (logit_fc): Sequential(\n",
            "      (0): Linear(in_features=768, out_features=1536, bias=True)\n",
            "      (1): GeLU()\n",
            "      (2): BertLayerNorm()\n",
            "      (3): Linear(in_features=1536, out_features=1533, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (classifier): SimpleClassifier(\n",
            "    (main): Sequential(\n",
            "      (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
            "      (1): Swish()\n",
            "      (2): Dropout(p=0.45, inplace=True)\n",
            "      (3): Linear(in_features=2048, out_features=1533, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "nParams=\t173967797\n",
            "optim: adamax lr=0.0007, decay_step=2, decay_rate=0.25, grad_clip=0.25\n",
            "gradual warmup lr: 0.0003\n",
            "/content/CFR_VQA/src/utils.py:146: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  storage = batch[0].storage()._new_shared(numel)\n",
            "/content/CFR_VQA/src/utils.py:149: UserWarning: An output with one or more elements was resized since it had shape [6553600], which does not match the required output shape [64, 50, 2048]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack([F.pad(x, (0, 0, 0, max_num_boxes-x.size(0))).data for x in batch], 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:149: UserWarning: An output with one or more elements was resized since it had shape [19200], which does not match the required output shape [64, 50, 6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack([F.pad(x, (0, 0, 0, max_num_boxes-x.size(0))).data for x in batch], 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:157: UserWarning: An output with one or more elements was resized since it had shape [1920], which does not match the required output shape [64, 30]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack(batch, 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:157: UserWarning: An output with one or more elements was resized since it had shape [448], which does not match the required output shape [64, 7]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack(batch, 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:149: UserWarning: An output with one or more elements was resized since it had shape [2880], which does not match the required output shape [64, 15, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack([F.pad(x, (0, 0, 0, max_num_boxes-x.size(0))).data for x in batch], 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:157: UserWarning: An output with one or more elements was resized since it had shape [768], which does not match the required output shape [64, 12]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack(batch, 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:157: UserWarning: An output with one or more elements was resized since it had shape [98112], which does not match the required output shape [64, 1533]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack(batch, 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:157: UserWarning: An output with one or more elements was resized since it had shape [128], which does not match the required output shape [64, 2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack(batch, 0, out=out)\n",
            "/content/CFR_VQA/lxrt/optimization.py:145: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha = 1) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1642.)\n",
            "  next_m.mul_(beta1).add_(1 - beta1, grad)\n",
            "/content/CFR_VQA/src/utils.py:149: UserWarning: An output with one or more elements was resized since it had shape [1638400], which does not match the required output shape [16, 50, 2048]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack([F.pad(x, (0, 0, 0, max_num_boxes-x.size(0))).data for x in batch], 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:149: UserWarning: An output with one or more elements was resized since it had shape [4800], which does not match the required output shape [16, 50, 6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack([F.pad(x, (0, 0, 0, max_num_boxes-x.size(0))).data for x in batch], 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:157: UserWarning: An output with one or more elements was resized since it had shape [480], which does not match the required output shape [16, 30]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack(batch, 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:157: UserWarning: An output with one or more elements was resized since it had shape [112], which does not match the required output shape [16, 7]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack(batch, 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:149: UserWarning: An output with one or more elements was resized since it had shape [720], which does not match the required output shape [16, 15, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack([F.pad(x, (0, 0, 0, max_num_boxes-x.size(0))).data for x in batch], 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:157: UserWarning: An output with one or more elements was resized since it had shape [192], which does not match the required output shape [16, 12]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack(batch, 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:157: UserWarning: An output with one or more elements was resized since it had shape [24528], which does not match the required output shape [16, 1533]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack(batch, 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:157: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [16, 2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack(batch, 0, out=out)\n",
            "Evaluating...\n",
            "/content/CFR_VQA/src/utils.py:149: UserWarning: An output with one or more elements was resized since it had shape [4718592], which does not match the required output shape [64, 36, 2048]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack([F.pad(x, (0, 0, 0, max_num_boxes-x.size(0))).data for x in batch], 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:149: UserWarning: An output with one or more elements was resized since it had shape [13824], which does not match the required output shape [64, 36, 6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack([F.pad(x, (0, 0, 0, max_num_boxes-x.size(0))).data for x in batch], 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:149: UserWarning: An output with one or more elements was resized since it had shape [4325376], which does not match the required output shape [64, 33, 2048]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack([F.pad(x, (0, 0, 0, max_num_boxes-x.size(0))).data for x in batch], 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:149: UserWarning: An output with one or more elements was resized since it had shape [12672], which does not match the required output shape [64, 33, 6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack([F.pad(x, (0, 0, 0, max_num_boxes-x.size(0))).data for x in batch], 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:149: UserWarning: An output with one or more elements was resized since it had shape [3801088], which does not match the required output shape [64, 29, 2048]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack([F.pad(x, (0, 0, 0, max_num_boxes-x.size(0))).data for x in batch], 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:149: UserWarning: An output with one or more elements was resized since it had shape [11136], which does not match the required output shape [64, 29, 6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack([F.pad(x, (0, 0, 0, max_num_boxes-x.size(0))).data for x in batch], 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:149: UserWarning: An output with one or more elements was resized since it had shape [5505024], which does not match the required output shape [64, 42, 2048]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack([F.pad(x, (0, 0, 0, max_num_boxes-x.size(0))).data for x in batch], 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:149: UserWarning: An output with one or more elements was resized since it had shape [16128], which does not match the required output shape [64, 42, 6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack([F.pad(x, (0, 0, 0, max_num_boxes-x.size(0))).data for x in batch], 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:149: UserWarning: An output with one or more elements was resized since it had shape [5111808], which does not match the required output shape [64, 39, 2048]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack([F.pad(x, (0, 0, 0, max_num_boxes-x.size(0))).data for x in batch], 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:149: UserWarning: An output with one or more elements was resized since it had shape [14976], which does not match the required output shape [64, 39, 6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack([F.pad(x, (0, 0, 0, max_num_boxes-x.size(0))).data for x in batch], 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:149: UserWarning: An output with one or more elements was resized since it had shape [5636096], which does not match the required output shape [64, 43, 2048]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack([F.pad(x, (0, 0, 0, max_num_boxes-x.size(0))).data for x in batch], 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:149: UserWarning: An output with one or more elements was resized since it had shape [16512], which does not match the required output shape [64, 43, 6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack([F.pad(x, (0, 0, 0, max_num_boxes-x.size(0))).data for x in batch], 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:149: UserWarning: An output with one or more elements was resized since it had shape [6422528], which does not match the required output shape [64, 49, 2048]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack([F.pad(x, (0, 0, 0, max_num_boxes-x.size(0))).data for x in batch], 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:149: UserWarning: An output with one or more elements was resized since it had shape [18816], which does not match the required output shape [64, 49, 6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack([F.pad(x, (0, 0, 0, max_num_boxes-x.size(0))).data for x in batch], 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:149: UserWarning: An output with one or more elements was resized since it had shape [3538944], which does not match the required output shape [64, 27, 2048]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack([F.pad(x, (0, 0, 0, max_num_boxes-x.size(0))).data for x in batch], 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:149: UserWarning: An output with one or more elements was resized since it had shape [10368], which does not match the required output shape [64, 27, 6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack([F.pad(x, (0, 0, 0, max_num_boxes-x.size(0))).data for x in batch], 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:149: UserWarning: An output with one or more elements was resized since it had shape [4980736], which does not match the required output shape [64, 38, 2048]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack([F.pad(x, (0, 0, 0, max_num_boxes-x.size(0))).data for x in batch], 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:149: UserWarning: An output with one or more elements was resized since it had shape [14592], which does not match the required output shape [64, 38, 6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack([F.pad(x, (0, 0, 0, max_num_boxes-x.size(0))).data for x in batch], 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:149: UserWarning: An output with one or more elements was resized since it had shape [6291456], which does not match the required output shape [64, 48, 2048]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack([F.pad(x, (0, 0, 0, max_num_boxes-x.size(0))).data for x in batch], 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:149: UserWarning: An output with one or more elements was resized since it had shape [18432], which does not match the required output shape [64, 48, 6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack([F.pad(x, (0, 0, 0, max_num_boxes-x.size(0))).data for x in batch], 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:149: UserWarning: An output with one or more elements was resized since it had shape [3276800], which does not match the required output shape [64, 25, 2048]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack([F.pad(x, (0, 0, 0, max_num_boxes-x.size(0))).data for x in batch], 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:149: UserWarning: An output with one or more elements was resized since it had shape [9600], which does not match the required output shape [64, 25, 6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack([F.pad(x, (0, 0, 0, max_num_boxes-x.size(0))).data for x in batch], 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:149: UserWarning: An output with one or more elements was resized since it had shape [4849664], which does not match the required output shape [64, 37, 2048]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack([F.pad(x, (0, 0, 0, max_num_boxes-x.size(0))).data for x in batch], 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:149: UserWarning: An output with one or more elements was resized since it had shape [14208], which does not match the required output shape [64, 37, 6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack([F.pad(x, (0, 0, 0, max_num_boxes-x.size(0))).data for x in batch], 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:149: UserWarning: An output with one or more elements was resized since it had shape [5898240], which does not match the required output shape [64, 45, 2048]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack([F.pad(x, (0, 0, 0, max_num_boxes-x.size(0))).data for x in batch], 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:149: UserWarning: An output with one or more elements was resized since it had shape [17280], which does not match the required output shape [64, 45, 6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack([F.pad(x, (0, 0, 0, max_num_boxes-x.size(0))).data for x in batch], 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:149: UserWarning: An output with one or more elements was resized since it had shape [6029312], which does not match the required output shape [64, 46, 2048]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack([F.pad(x, (0, 0, 0, max_num_boxes-x.size(0))).data for x in batch], 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:149: UserWarning: An output with one or more elements was resized since it had shape [17664], which does not match the required output shape [64, 46, 6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack([F.pad(x, (0, 0, 0, max_num_boxes-x.size(0))).data for x in batch], 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:149: UserWarning: An output with one or more elements was resized since it had shape [5242880], which does not match the required output shape [64, 40, 2048]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack([F.pad(x, (0, 0, 0, max_num_boxes-x.size(0))).data for x in batch], 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:149: UserWarning: An output with one or more elements was resized since it had shape [15360], which does not match the required output shape [64, 40, 6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack([F.pad(x, (0, 0, 0, max_num_boxes-x.size(0))).data for x in batch], 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:149: UserWarning: An output with one or more elements was resized since it had shape [4063232], which does not match the required output shape [64, 31, 2048]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack([F.pad(x, (0, 0, 0, max_num_boxes-x.size(0))).data for x in batch], 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:149: UserWarning: An output with one or more elements was resized since it had shape [11904], which does not match the required output shape [64, 31, 6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack([F.pad(x, (0, 0, 0, max_num_boxes-x.size(0))).data for x in batch], 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:149: UserWarning: An output with one or more elements was resized since it had shape [3014656], which does not match the required output shape [64, 23, 2048]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack([F.pad(x, (0, 0, 0, max_num_boxes-x.size(0))).data for x in batch], 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:149: UserWarning: An output with one or more elements was resized since it had shape [8832], which does not match the required output shape [64, 23, 6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack([F.pad(x, (0, 0, 0, max_num_boxes-x.size(0))).data for x in batch], 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:149: UserWarning: An output with one or more elements was resized since it had shape [4194304], which does not match the required output shape [64, 32, 2048]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack([F.pad(x, (0, 0, 0, max_num_boxes-x.size(0))).data for x in batch], 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:149: UserWarning: An output with one or more elements was resized since it had shape [12288], which does not match the required output shape [64, 32, 6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack([F.pad(x, (0, 0, 0, max_num_boxes-x.size(0))).data for x in batch], 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:149: UserWarning: An output with one or more elements was resized since it had shape [4456448], which does not match the required output shape [64, 34, 2048]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack([F.pad(x, (0, 0, 0, max_num_boxes-x.size(0))).data for x in batch], 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:149: UserWarning: An output with one or more elements was resized since it had shape [13056], which does not match the required output shape [64, 34, 6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack([F.pad(x, (0, 0, 0, max_num_boxes-x.size(0))).data for x in batch], 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:149: UserWarning: An output with one or more elements was resized since it had shape [6160384], which does not match the required output shape [64, 47, 2048]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack([F.pad(x, (0, 0, 0, max_num_boxes-x.size(0))).data for x in batch], 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:149: UserWarning: An output with one or more elements was resized since it had shape [18048], which does not match the required output shape [64, 47, 6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack([F.pad(x, (0, 0, 0, max_num_boxes-x.size(0))).data for x in batch], 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:149: UserWarning: An output with one or more elements was resized since it had shape [5767168], which does not match the required output shape [64, 44, 2048]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack([F.pad(x, (0, 0, 0, max_num_boxes-x.size(0))).data for x in batch], 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:149: UserWarning: An output with one or more elements was resized since it had shape [16896], which does not match the required output shape [64, 44, 6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack([F.pad(x, (0, 0, 0, max_num_boxes-x.size(0))).data for x in batch], 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:149: UserWarning: An output with one or more elements was resized since it had shape [5373952], which does not match the required output shape [64, 41, 2048]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack([F.pad(x, (0, 0, 0, max_num_boxes-x.size(0))).data for x in batch], 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:149: UserWarning: An output with one or more elements was resized since it had shape [15744], which does not match the required output shape [64, 41, 6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack([F.pad(x, (0, 0, 0, max_num_boxes-x.size(0))).data for x in batch], 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:149: UserWarning: An output with one or more elements was resized since it had shape [4587520], which does not match the required output shape [64, 35, 2048]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack([F.pad(x, (0, 0, 0, max_num_boxes-x.size(0))).data for x in batch], 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:149: UserWarning: An output with one or more elements was resized since it had shape [13440], which does not match the required output shape [64, 35, 6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack([F.pad(x, (0, 0, 0, max_num_boxes-x.size(0))).data for x in batch], 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:149: UserWarning: An output with one or more elements was resized since it had shape [3932160], which does not match the required output shape [64, 30, 2048]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack([F.pad(x, (0, 0, 0, max_num_boxes-x.size(0))).data for x in batch], 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:149: UserWarning: An output with one or more elements was resized since it had shape [11520], which does not match the required output shape [64, 30, 6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack([F.pad(x, (0, 0, 0, max_num_boxes-x.size(0))).data for x in batch], 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:149: UserWarning: An output with one or more elements was resized since it had shape [1277952], which does not match the required output shape [16, 39, 2048]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack([F.pad(x, (0, 0, 0, max_num_boxes-x.size(0))).data for x in batch], 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:149: UserWarning: An output with one or more elements was resized since it had shape [3744], which does not match the required output shape [16, 39, 6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack([F.pad(x, (0, 0, 0, max_num_boxes-x.size(0))).data for x in batch], 0, out=out)\n",
            "epoch 0, time: 110.75\n",
            "\ttrain_loss: 235.82, norm: 1266.6398, score: 16.91, question type score: 0.00\n",
            "\tCFRF score: 17.70 (99.92)\n",
            "gradual warmup lr: 0.0007\n",
            "Evaluating...\n",
            "epoch 1, time: 113.80\n",
            "\ttrain_loss: 9.91, norm: 10.3826, score: 24.34, question type score: 0.00\n",
            "\tCFRF score: 29.95 (99.92)\n",
            "gradual warmup lr: 0.0010\n",
            "Evaluating...\n",
            "epoch 2, time: 114.12\n",
            "\ttrain_loss: 7.42, norm: 8.8054, score: 34.88, question type score: 0.00\n",
            "\tCFRF score: 35.99 (99.92)\n",
            "gradual warmup lr: 0.0014\n",
            "Evaluating...\n",
            "epoch 3, time: 113.97\n",
            "\ttrain_loss: 6.33, norm: 8.9505, score: 43.28, question type score: 0.00\n",
            "\tCFRF score: 42.07 (99.92)\n",
            "lr: 0.0014\n",
            "Evaluating...\n",
            "epoch 4, time: 113.85\n",
            "\ttrain_loss: 5.46, norm: 8.8538, score: 58.79, question type score: 0.00\n",
            "\tCFRF score: 44.51 (99.92)\n",
            "lr: 0.0014\n",
            "Evaluating...\n",
            "epoch 5, time: 114.12\n",
            "\ttrain_loss: 4.46, norm: 8.6461, score: 72.07, question type score: 0.00\n",
            "\tCFRF score: 45.18 (99.92)\n",
            "lr: 0.0014\n",
            "Evaluating...\n",
            "epoch 6, time: 113.82\n",
            "\ttrain_loss: 3.89, norm: 8.6673, score: 81.45, question type score: 0.00\n",
            "\tCFRF score: 46.58 (99.92)\n",
            "lr: 0.0014\n",
            "Evaluating...\n",
            "epoch 7, time: 114.00\n",
            "\ttrain_loss: 3.39, norm: 8.7319, score: 87.93, question type score: 0.00\n",
            "\tCFRF score: 47.40 (99.92)\n",
            "lr: 0.0014\n",
            "Evaluating...\n",
            "epoch 8, time: 113.70\n",
            "\ttrain_loss: 3.04, norm: 8.5472, score: 90.43, question type score: 0.00\n",
            "\tCFRF score: 46.94 (99.92)\n",
            "lr: 0.0014\n",
            "/content/CFR_VQA/src/utils.py:149: UserWarning: An output with one or more elements was resized since it had shape [1605632], which does not match the required output shape [16, 49, 2048]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack([F.pad(x, (0, 0, 0, max_num_boxes-x.size(0))).data for x in batch], 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:149: UserWarning: An output with one or more elements was resized since it had shape [4704], which does not match the required output shape [16, 49, 6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack([F.pad(x, (0, 0, 0, max_num_boxes-x.size(0))).data for x in batch], 0, out=out)\n",
            "Evaluating...\n",
            "epoch 9, time: 113.84\n",
            "\ttrain_loss: 2.67, norm: 8.3477, score: 93.48, question type score: 0.00\n",
            "\tCFRF score: 47.51 (99.92)\n",
            "decreased lr: 0.0003\n",
            "Evaluating...\n",
            "epoch 10, time: 114.16\n",
            "\ttrain_loss: 2.35, norm: 8.5334, score: 94.61, question type score: 0.00\n",
            "\tCFRF score: 46.97 (99.92)\n",
            "lr: 0.0003\n",
            "/content/CFR_VQA/src/utils.py:149: UserWarning: An output with one or more elements was resized since it had shape [1376256], which does not match the required output shape [16, 42, 2048]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack([F.pad(x, (0, 0, 0, max_num_boxes-x.size(0))).data for x in batch], 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:149: UserWarning: An output with one or more elements was resized since it had shape [4032], which does not match the required output shape [16, 42, 6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack([F.pad(x, (0, 0, 0, max_num_boxes-x.size(0))).data for x in batch], 0, out=out)\n",
            "Evaluating...\n",
            "epoch 11, time: 113.74\n",
            "\ttrain_loss: 2.17, norm: 8.5860, score: 95.70, question type score: 0.00\n",
            "\tCFRF score: 47.02 (99.92)\n",
            "decreased lr: 0.0001\n",
            "Evaluating...\n",
            "epoch 12, time: 113.87\n",
            "\ttrain_loss: 2.01, norm: 8.1834, score: 96.64, question type score: 0.00\n",
            "\tCFRF score: 47.49 (99.92)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash test.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7Hf79TyBRW5",
        "outputId": "21fcb199-7735-45b5-f51c-d9bdcc8bd406"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluate a given model optimized by training split using validation split.\n",
            "Namespace(split='val', input='saved_models/GQA/replicate_result_default', output='results/GQA', epoch='_best', batch_size=256, model='CFRF_Model', gamma=2, use_counter=False, counter_act='zhang', debug=False, gpu=0, max_boxes=40, question_len=12, num_stat_word=30, op='c', num_hid=1024, activation='swish', dropout=0.45, dataset='GQA', tiny=False, load=None, load_lxmert=None, load_lxmert_qa=None, mce_loss=False, llayers=7, xlayers=2, rlayers=2, task_matched=False, task_mask_lm=False, task_obj_predict=False, task_qa=False, visual_losses='obj,attr,feat', qa_sets=None, word_mask_rate=0.15, obj_mask_rate=0.15, multiGPU=False, num_workers=0, omega_q=0.001, omega_v=0.001, topk='6')\n",
            "loading dictionary from data/gqa/dictionary.pkl\n",
            "Create val entries\n",
            "loading features from h5 file data/gqa/val.hdf5 \n",
            "LXRT encoder with 7 l_layers, 2 x_layers, and 2 r_layers.\n",
            "/content/CFR_VQA/lxrt/modeling.py:843: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(weights_path, map_location='cpu' if not torch.cuda.is_available() else None)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
            "  WeightNorm.apply(module, name, dim)\n",
            "CFRF_Model(\n",
            "  (w_emb): WordEmbedding(\n",
            "    (emb): Embedding(2932, 300, padding_idx=2931)\n",
            "    (emb_): Embedding(2932, 300, padding_idx=2931)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (q_emb): QuestionEmbedding(\n",
            "    (rnn): GRU(600, 1024, batch_first=True)\n",
            "  )\n",
            "  (sw_emb): WordEmbedding(\n",
            "    (emb): Embedding(2932, 300, padding_idx=2931)\n",
            "    (emb_): Embedding(2932, 300, padding_idx=2931)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (s_emb): QuestionEmbedding(\n",
            "    (rnn): GRU(600, 1024, batch_first=True)\n",
            "  )\n",
            "  (ew_emb): WordEmbedding(\n",
            "    (emb): Embedding(2932, 300, padding_idx=2931)\n",
            "    (emb_): Embedding(2932, 300, padding_idx=2931)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (qe_joint): BanFusion(\n",
            "    (v_att): BiAttention(\n",
            "      (logits): BCNet(\n",
            "        (v_net): FCNet(\n",
            "          (main): Sequential(\n",
            "            (0): Dropout(p=0.2, inplace=False)\n",
            "            (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (2): ReLU()\n",
            "          )\n",
            "        )\n",
            "        (q_net): FCNet(\n",
            "          (main): Sequential(\n",
            "            (0): Dropout(p=0.2, inplace=False)\n",
            "            (1): Linear(in_features=600, out_features=1024, bias=True)\n",
            "            (2): ReLU()\n",
            "          )\n",
            "        )\n",
            "        (dropout): Dropout(p=0.5, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (b_net): ModuleList(\n",
            "      (0-1): 2 x BCNet(\n",
            "        (v_net): FCNet(\n",
            "          (main): Sequential(\n",
            "            (0): Dropout(p=0.2, inplace=False)\n",
            "            (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (2): ReLU()\n",
            "          )\n",
            "        )\n",
            "        (q_net): FCNet(\n",
            "          (main): Sequential(\n",
            "            (0): Dropout(p=0.2, inplace=False)\n",
            "            (1): Linear(in_features=600, out_features=1024, bias=True)\n",
            "            (2): ReLU()\n",
            "          )\n",
            "        )\n",
            "        (dropout): Dropout(p=0.5, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (q_prj): ModuleList(\n",
            "      (0-1): 2 x FCNet(\n",
            "        (main): Sequential(\n",
            "          (0): Dropout(p=0.2, inplace=False)\n",
            "          (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (vs_joint): BanFusion(\n",
            "    (v_att): BiAttention(\n",
            "      (logits): BCNet(\n",
            "        (v_net): FCNet(\n",
            "          (main): Sequential(\n",
            "            (0): Dropout(p=0.2, inplace=False)\n",
            "            (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (2): ReLU()\n",
            "          )\n",
            "        )\n",
            "        (q_net): FCNet(\n",
            "          (main): Sequential(\n",
            "            (0): Dropout(p=0.2, inplace=False)\n",
            "            (1): Linear(in_features=2048, out_features=1024, bias=True)\n",
            "            (2): ReLU()\n",
            "          )\n",
            "        )\n",
            "        (dropout): Dropout(p=0.5, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (b_net): ModuleList(\n",
            "      (0-1): 2 x BCNet(\n",
            "        (v_net): FCNet(\n",
            "          (main): Sequential(\n",
            "            (0): Dropout(p=0.2, inplace=False)\n",
            "            (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (2): ReLU()\n",
            "          )\n",
            "        )\n",
            "        (q_net): FCNet(\n",
            "          (main): Sequential(\n",
            "            (0): Dropout(p=0.2, inplace=False)\n",
            "            (1): Linear(in_features=2048, out_features=1024, bias=True)\n",
            "            (2): ReLU()\n",
            "          )\n",
            "        )\n",
            "        (dropout): Dropout(p=0.5, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (q_prj): ModuleList(\n",
            "      (0-1): 2 x FCNet(\n",
            "        (main): Sequential(\n",
            "          (0): Dropout(p=0.2, inplace=False)\n",
            "          (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (vq_joint): BanFusion(\n",
            "    (v_att): BiAttention(\n",
            "      (logits): BCNet(\n",
            "        (v_net): FCNet(\n",
            "          (main): Sequential(\n",
            "            (0): Dropout(p=0.2, inplace=False)\n",
            "            (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (2): ReLU()\n",
            "          )\n",
            "        )\n",
            "        (q_net): FCNet(\n",
            "          (main): Sequential(\n",
            "            (0): Dropout(p=0.2, inplace=False)\n",
            "            (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (2): ReLU()\n",
            "          )\n",
            "        )\n",
            "        (dropout): Dropout(p=0.5, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (b_net): ModuleList(\n",
            "      (0-1): 2 x BCNet(\n",
            "        (v_net): FCNet(\n",
            "          (main): Sequential(\n",
            "            (0): Dropout(p=0.2, inplace=False)\n",
            "            (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (2): ReLU()\n",
            "          )\n",
            "        )\n",
            "        (q_net): FCNet(\n",
            "          (main): Sequential(\n",
            "            (0): Dropout(p=0.2, inplace=False)\n",
            "            (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (2): ReLU()\n",
            "          )\n",
            "        )\n",
            "        (dropout): Dropout(p=0.5, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (q_prj): ModuleList(\n",
            "      (0-1): 2 x FCNet(\n",
            "        (main): Sequential(\n",
            "          (0): Dropout(p=0.2, inplace=False)\n",
            "          (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (lxmert_encoder): Lxmert_Model(\n",
            "    (lxrt_encoder): LXRTEncoder(\n",
            "      (model): LXRTFeatureExtraction(\n",
            "        (bert): LXRTModel(\n",
            "          (embeddings): BertEmbeddings(\n",
            "            (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "            (position_embeddings): Embedding(512, 768, padding_idx=0)\n",
            "            (token_type_embeddings): Embedding(2, 768, padding_idx=0)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (encoder): LXRTEncoder(\n",
            "            (visn_fc): VisualFeatEncoder(\n",
            "              (visn_fc): Linear(in_features=2048, out_features=768, bias=True)\n",
            "              (visn_layer_norm): BertLayerNorm()\n",
            "              (box_fc): Linear(in_features=4, out_features=768, bias=True)\n",
            "              (box_layer_norm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (layer): ModuleList(\n",
            "              (0-6): 7 x BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): BertLayerNorm()\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): BertLayerNorm()\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (x_layers): ModuleList(\n",
            "              (0-1): 2 x LXRTXLayer(\n",
            "                (lang_self_att): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): BertLayerNorm()\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (lang_inter): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (lang_output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): BertLayerNorm()\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (visn_self_att): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): BertLayerNorm()\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (visn_inter): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (visn_output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): BertLayerNorm()\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (visual_attention): BertXAttention(\n",
            "                  (att): BertOutAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): BertLayerNorm()\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (r_layers): ModuleList(\n",
            "              (0-1): 2 x BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): BertLayerNorm()\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): BertLayerNorm()\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (pooler): BertPooler(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (activation): Tanh()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (logit_fc): Sequential(\n",
            "      (0): Linear(in_features=768, out_features=1536, bias=True)\n",
            "      (1): GeLU()\n",
            "      (2): BertLayerNorm()\n",
            "      (3): Linear(in_features=1536, out_features=1533, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (classifier): SimpleClassifier(\n",
            "    (main): Sequential(\n",
            "      (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
            "      (1): Swish()\n",
            "      (2): Dropout(p=0.45, inplace=True)\n",
            "      (3): Linear(in_features=2048, out_features=1533, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "loading saved_models/GQA/replicate_result_default/model_epoch_best.pth\n",
            "/content/CFR_VQA/evaluate.py:148: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model_data = torch.load(model_path, map_location=args.device)\n",
            "Evaluating...\n",
            "/content/CFR_VQA/src/utils.py:146: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  storage = batch[0].storage()._new_shared(numel)\n",
            "/content/CFR_VQA/src/utils.py:149: UserWarning: An output with one or more elements was resized since it had shape [20971520], which does not match the required output shape [256, 40, 2048]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack([F.pad(x, (0, 0, 0, max_num_boxes-x.size(0))).data for x in batch], 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:149: UserWarning: An output with one or more elements was resized since it had shape [61440], which does not match the required output shape [256, 40, 6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack([F.pad(x, (0, 0, 0, max_num_boxes-x.size(0))).data for x in batch], 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:157: UserWarning: An output with one or more elements was resized since it had shape [7680], which does not match the required output shape [256, 30]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack(batch, 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:157: UserWarning: An output with one or more elements was resized since it had shape [1792], which does not match the required output shape [256, 7]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack(batch, 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:149: UserWarning: An output with one or more elements was resized since it had shape [11520], which does not match the required output shape [256, 15, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack([F.pad(x, (0, 0, 0, max_num_boxes-x.size(0))).data for x in batch], 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:157: UserWarning: An output with one or more elements was resized since it had shape [3072], which does not match the required output shape [256, 12]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack(batch, 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:157: UserWarning: An output with one or more elements was resized since it had shape [392448], which does not match the required output shape [256, 1533]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack(batch, 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:157: UserWarning: An output with one or more elements was resized since it had shape [512], which does not match the required output shape [256, 2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack(batch, 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:149: UserWarning: An output with one or more elements was resized since it had shape [18874368], which does not match the required output shape [256, 36, 2048]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack([F.pad(x, (0, 0, 0, max_num_boxes-x.size(0))).data for x in batch], 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:149: UserWarning: An output with one or more elements was resized since it had shape [55296], which does not match the required output shape [256, 36, 6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack([F.pad(x, (0, 0, 0, max_num_boxes-x.size(0))).data for x in batch], 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:149: UserWarning: An output with one or more elements was resized since it had shape [1277952], which does not match the required output shape [16, 39, 2048]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack([F.pad(x, (0, 0, 0, max_num_boxes-x.size(0))).data for x in batch], 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:149: UserWarning: An output with one or more elements was resized since it had shape [3744], which does not match the required output shape [16, 39, 6]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack([F.pad(x, (0, 0, 0, max_num_boxes-x.size(0))).data for x in batch], 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:157: UserWarning: An output with one or more elements was resized since it had shape [480], which does not match the required output shape [16, 30]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack(batch, 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:157: UserWarning: An output with one or more elements was resized since it had shape [112], which does not match the required output shape [16, 7]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack(batch, 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:149: UserWarning: An output with one or more elements was resized since it had shape [720], which does not match the required output shape [16, 15, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack([F.pad(x, (0, 0, 0, max_num_boxes-x.size(0))).data for x in batch], 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:157: UserWarning: An output with one or more elements was resized since it had shape [192], which does not match the required output shape [16, 12]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack(batch, 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:157: UserWarning: An output with one or more elements was resized since it had shape [24528], which does not match the required output shape [16, 1533]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack(batch, 0, out=out)\n",
            "/content/CFR_VQA/src/utils.py:157: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [16, 2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  return torch.stack(batch, 0, out=out)\n",
            "\tCFRF score: 47.55 (99.92)\n"
          ]
        }
      ]
    }
  ]
}